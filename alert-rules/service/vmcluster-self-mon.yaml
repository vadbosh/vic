groups:
  - name: VictoriaMetrics.self.mon.rules
    rules:

      # ============================================================
      # 1. AVAILABILITY & RESTARTS
      # ============================================================

      # FIXED: Use resets() instead of changes() and increased threshold
      # resets() accurately detects restarts while ignoring data gaps
      # Why changes() was problematic:
      # - Counts any value change, including scrape timeouts
      # - HA setup with 2 replicas makes it sensitive to temporary gaps
      # - Network blips would trigger false positives
      - alert: VictoriaMetricsTooManyRestarts
        expr: increase(kube_pod_container_status_restarts_total{exported_container=~"^vm.*|alertmanager"}[15m]) > 2
        for: 2m  # ADDED: Protection against brief spikes
        labels:
          severity: warning
        annotations:
          summary: "Instance {{ $labels.exported_pod}} restarting too often"
          description: "The service {{ $labels.exported_pod }} has restarted {{ $value }} times in the last 15 minutes."

      # NEW: Detection of complete component disappearance
      # Distinguishes between "down" (exists but not responding) vs "disappeared" (removed from discovery)
      # Logic: Shows instances that existed 10m ago but do not exist now
      - alert: VictoriaMetricsInstanceDisappeared
        expr: |
          up{job=~"vmagent|vmauth|vminsert|vmselect|vmstorage"} offset 10m
          unless
          up{job=~"vmagent|vmauth|vminsert|vmselect|vmstorage"}
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Instance {{ $labels.instance }} ({{ $labels.job }}) has disappeared"
          description: "The instance was present 10 minutes ago but is missing now. Check if the service/pod crashed or was removed."

      # Component exists in service discovery but not responding (up=0)
      # This usually indicates network issues, pod crash, or process hang
      - alert: VictoriaMetricsTargetDown
        expr: up{job=~"vmagent|vmauth|vminsert|vmselect|vmstorage"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Target {{ $labels.instance }} is down"
          description: "Target is discovered but cannot be scraped. Check network connectivity or service status."

      # ============================================================
      # 2. VMAGENT (Scraping Health)
      # ============================================================

      # FIXED: Added threshold and for clause
      # Now triggers only if >5% of scrapes fail consistently
      # Why percentage-based:
      # - Absolute count can be misleading (1 error in 10000 scrapes is fine)
      # - Percentage adapts to scrape volume
      # - 5% threshold allows for transient network issues
      - alert: VMAgentScrapeFailedHigh
        expr: |
          (
           sum(rate(vm_promscrape_scrapes_failed_total[5m])) by (instance, job)
           /
           sum(rate(vm_promscrape_scrapes_total[5m])) by (instance, job)
          ) > 0.05
        for: 5m  # ADDED: Only fires if problem persists for 10+ minutes
        labels:
          severity: warning
        annotations:
          summary: "vmagent scrapes are failing ({{ $labels.job }})"
          description: "{{ $value | humanizePercentage }} of scrapes are failing on {{ $labels.job }}. Check logs for specific target errors."

      # NEW: Alert for absolute number of failed scrapes
      # Useful when percentage is low but absolute number is high
      # Example: 1% of 100k scrapes = 1k failures = potential issue
      - alert: VMAgentScrapeFailedCount
        expr: rate(vm_promscrape_scrapes_failed_total[5m]) > 10
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High number of failed scrapes on {{ $labels.instance }}"
          description: "{{ $value | humanize }} scrapes/sec are failing. This may indicate target connectivity issues."

      # ============================================================
      # 3. VMSTORAGE (Data Ingestion Health)
      # ============================================================

      # CRITICAL: No data is being added to storage
      # This is a cluster-wide problem indicating complete write path failure
      # Possible causes:
      # - All vminsert pods down
      # - All vmagent pods down
      # - Network partition between vmagent and vminsert
      - alert: VMStorageIngestionStopped
        expr: sum(rate(vm_rows_added_to_storage_total[5m])) == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "vmstorage cluster is not ingesting data"
          description: "The rate of rows added to storage is 0. Data ingestion has stopped completely. Check vmagent and vminsert status."

      # Per-node ingestion monitoring
      # Each vmstorage node should receive data
      # If one node receives 0, check vminsert routing logic
      - alert: VMStorageNodeNotReceivingData
        expr: rate(vm_rows_added_to_storage_total{job="vmstorage"}[5m]) == 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "vmstorage node {{ $labels.instance }} is not receiving data"
          description: "This node is not receiving any data. Check vminsert routing and network connectivity."

      # ============================================================
      # 4. DEDUPLICATION MONITORING
      # ============================================================

      # FIXED: Significantly increased threshold and added for clause
      # 67k samples/sec on read is a problem, but needs stability
      # Why this happens:
      # 1. HA setup: 2 vmagent replicas → duplicate data
      # 2. If query covers large time range, vmselect deduplicates millions of points
      # 3. 67k samples/sec = normal for burst query, but threshold 1000 was too low
      # Expected behavior:
      # - Small queries: <1k samples/sec dedup on read
      # - Large dashboard refresh: 10-50k samples/sec spike (temporary)
      # - Persistent >50k: real problem (data not deduped during write)
      - alert: HighSelectDeduplication
        expr: sum(rate(vm_deduplicated_samples_total{job="vmselect", type="select"}[5m])) > 50000
        for: 15m  # INCREASED: Ignores temporary query bursts
        labels:
          severity: warning
          alertname: SelectDedup
        annotations:
          summary: "High deduplication rate on read (vmselect)"
          description: |
            vmselect is deduplicating {{ $value | humanize }} samples/sec on read for 15+ minutes.
            This indicates deduplication is happening during queries instead of during write.
            Possible causes:
            - dedup.minScrapeInterval mismatch between vmstorage and vmselect
            - Data written before deduplication was enabled
            - Multiple vmagent instances with incorrect external_labels
            - replicationFactor configuration mismatch

      # NEW: Monitor deduplication spikes
      # Useful for understanding query patterns and identifying heavy queries
      # Fires only on 10x sudden increase (not for gradual growth)
      - alert: SelectDeduplicationSpike
        expr: |
          (
            sum(rate(vm_deduplicated_samples_total{job="vmselect", type="select"}[1m]))
            /
            sum(rate(vm_deduplicated_samples_total{job="vmselect", type="select"}[1m] offset 5m))
          ) > 10
        for: 5m
        labels:
          severity: info
        annotations:
          summary: "Sudden spike in vmselect deduplication"
          description: |
            Deduplication rate increased 10x compared to 5 minutes ago.
            Current rate: {{ $value | humanize }}x baseline. This may indicate a large query, dashboard refresh, or data backfill.

      # ============================================================
      # 5. WRITE PATH DEDUPLICATION HEALTH
      # ============================================================

      # Check that deduplication works on write path
      # In HA setup with 2 vmagent replicas, dedup rate should be ~50%
      # If 0% → only one vmagent is active (HA lost)
      # If 100% → configuration error (too aggressive dedup)
      - alert: DeduplicationRateLow
        expr: |
          (
            sum(rate(vm_deduplicated_samples_total{job="vmstorage", type="merge"}[5m]))
            /
            sum(rate(vm_rows_added_to_storage_total{job="vmstorage"}[5m]))
          ) < 0.40
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low deduplication rate on write path"
          description: |
            Current deduplication rate: {{ $value | humanizePercentage }}
            Expected: ~50% (0.50) for 2 HA vmagent replicas.
            If < 40%, check if one of the vmagent replicas is down or not sending data.

      # Detect complete deduplication stoppage
      # If samples are being written but deduplication is exactly 0, something is wrong
      # Causes: split-brain, config error, single active vmagent
      - alert: StorageDeduplicationNotWorking
        expr: |
          sum(rate(vm_deduplicated_samples_total{job="vmstorage", type="merge"}[10m])) == 0
          and
          sum(rate(vm_rows_added_to_storage_total{job="vmstorage"}[10m])) > 100
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Storage deduplication stopped (Loss of HA redundancy)"
          description: |
            Cluster is receiving data but deduplication is not working.
            Likely causes:
            - Only one vmagent replica is active (HA lost)
            - 'dedup.minScrapeInterval' is misconfigured
            - replicationFactor mismatch between vminsert and vmstorage

      # Detect deduplication imbalance between vmstorage nodes
      # Uses 'sum by (instance)' to aggregate internal metric types before comparing
      # Large variance indicates uneven load distribution
      - alert: DeduplicationImbalance
        expr: |
          (
            max(sum(rate(vm_deduplicated_samples_total{job="vmstorage", type="merge"}[5m])) by (instance))
            -
            min(sum(rate(vm_deduplicated_samples_total{job="vmstorage", type="merge"}[5m])) by (instance))
          )
          /
          avg(sum(rate(vm_deduplicated_samples_total{job="vmstorage", type="merge"}[5m])) by (instance))
          > 0.30
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Deduplication imbalance between vmstorage nodes"
          description: |
            Variance is > 30% (current: {{ $value | humanizePercentage }}).
            Check load balancing and replication status.
            This may indicate vminsert is not distributing data evenly.

      # ============================================================
      # 6. STORAGE CAPACITY & BALANCE
      # ============================================================

      # Monitor disk space on vmstorage nodes
      # 5GB threshold provides time to react before critical
      - alert: StorageDiskSpaceLow
        expr: vm_free_disk_space_bytes{job="vmstorage"} < 5368709120
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on vmstorage {{ $labels.instance }}"
          description: |
            Instance {{ $labels.instance }} has {{ $value | humanize1024 }}B free.
            Consider increasing volume size or reducing retention period.

      # Critical disk space alert
      # 2GB threshold = immediate action required
      - alert: StorageDiskSpaceCritical
        expr: vm_free_disk_space_bytes{job="vmstorage"} < 2147483648
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk space on vmstorage {{ $labels.instance }}"
          description: |
            Instance {{ $labels.instance }} has only {{ $value | humanize1024 }}B free.
            Data ingestion may fail soon. Immediate action required!

      # Monitor data size imbalance between nodes
      # Uses 'sum by (instance)' to combine all data types (indexdb, storage/big, storage/small)
      # >20% imbalance may indicate routing issues in vminsert
      - alert: StorageSizeImbalance
        expr: |
          (
            max(sum(vm_data_size_bytes{job="vmstorage"}) by (instance))
            -
            min(sum(vm_data_size_bytes{job="vmstorage"}) by (instance))
          )
          /
          avg(sum(vm_data_size_bytes{job="vmstorage"}) by (instance))
          > 0.20
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Data size imbalance detected"
          description: |
            Size difference > 20% (ratio: {{ $value | humanizePercentage }}).
            Calculated based on total data size per instance.
            This may indicate issues with vminsert hash-based routing.

      # ============================================================
      # 7. CLUSTER AVAILABILITY
      # ============================================================

      # Monitor active vmstorage node count
      # Uses 'or vector(0)' to ensure alert fires even if all pods are down
      # With replicationFactor=3, need all 3 nodes for full redundancy
      - alert: VMStorageCountMismatch
        expr: (count(up{job="vmstorage"} == 1) or vector(0)) < 3
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Not enough vmstorage nodes running"
          description: |
            Active nodes: {{ $value }}. Expected: 3.
            Required for full replicationFactor=3 coverage.
            Data loss risk if additional nodes fail.

      # Monitor vmselect availability
      # At least 1 vmselect must be available for queries
      - alert: VMSelectAllDown
        expr: (count(up{job="vmselect"} == 1) or vector(0)) == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "All vmselect instances are down"
          description: "No vmselect instances available. Query API is completely unavailable."

      # Monitor vminsert availability
      # At least 1 vminsert must be available for writes
      - alert: VMInsertAllDown
        expr: (count(up{job="vminsert"} == 1) or vector(0)) == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "All vminsert instances are down"
          description: "No vminsert instances available. Data ingestion is completely stopped."

      # ============================================================
      # 8. QUERY PERFORMANCE
      # ============================================================

      # Monitor slow queries (queries taking >10s as defined in vmselect config)
      # High rate of slow queries indicates:
      # - Inefficient queries (large time ranges, high cardinality selectors)
      # - Resource exhaustion (CPU/memory)
      # - Storage performance issues
      - alert: VMSelectSlowQueries
        expr: sum(rate(vm_slow_queries_total{job="vmselect"}[5m])) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "vmselect experiencing slow queries"
          description: "{{ $value | humanize }} slow queries/sec. Check query complexity and available resources."

      # Monitor query duration P99
      # High percentile indicates consistent performance issues
      - alert: VMSelectHighQueryLatency
        expr: |
          vm_request_duration_seconds{job="vmselect", quantile="0.99"} > 10
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "vmselect high query latency"
          description: "P99 query duration is {{ $value }}s. Queries are consistently slow."

      # ============================================================
      # 9. RESOURCE MONITORING
      # ============================================================

      # Monitor memory usage of vmstorage
      # >85% memory usage = risk of OOM killer
      - alert: VMStorageHighMemoryUsage
        expr: |
          label_replace(
          process_resident_memory_bytes{job="vmstorage"},
          "pod", "$1", "instance", "([^.]+).*"
          )
          / on(pod) group_left
            sum by (pod) (
            container_spec_memory_limit_bytes{pod=~"vm-cluster.*vmstorage.*"}
          ) > 0.85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "vmstorage {{ $labels.pod }} high memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}. Risk of OOM."

      # Monitor CPU throttling on vmstorage
      # High throttling = pod needs more CPU resources
      - alert: VMStorageCPUThrottling
        expr: |
          rate(container_cpu_cfs_throttled_seconds_total{pod=~"vm-cluster.*vmstorage.*"}[5m]) > 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "vmstorage {{ $labels.pod }} experiencing CPU throttling"
          description: "Pod is being throttled {{ $value }}s per second. Consider increasing CPU limits."

      # ============================================================
      # 10. DATA QUALITY & CONSISTENCY
      # ============================================================

      # Monitor merge speed
      # Slow merges can cause disk space issues
      - alert: VMStorageSlowMerge
        expr: |
          (
            sum(vm_rows_merged_total{job="vmstorage"}) by (instance)
            /
            sum(vm_rows_added_to_storage_total{job="vmstorage"}) by (instance)
          ) < 0.1
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "vmstorage {{ $labels.instance }} slow merge rate"
          description: |
            Merge rate is only {{ $value | humanizePercentage }} of write rate.
            This may lead to disk space issues and query performance degradation.

